# LLaMA Model Configuration
# Alternative option with strong instruction following

model:
  provider: "meta"
  model_id: "meta-llama/Llama-2-7b-chat-hf"
  model_type: "causal_lm"

  # Model capabilities
  capabilities:
    instruction_following: "high"
    context_length: 4096
    json_compliance: "medium-high"
    license: "llama-2-community"

  # Loading configuration
  loading:
    trust_remote_code: false
    torch_dtype: "auto"
    device_map: "auto"
    low_cpu_mem_usage: true
    use_auth_token: true  # Requires HF token for gated models

  # Attention optimization
  attention:
    use_flash_attention_2: true
    use_sdpa: true

  # Generation defaults
  generation:
    max_new_tokens: 800
    temperature: 0.2
    top_p: 0.9
    top_k: 50
    repetition_penalty: 1.1
    do_sample: true
    pad_token_id: 0
    eos_token_id: 2

  # Alternative LLaMA models
  alternatives:
    - model_id: "meta-llama/Llama-3-8B-Instruct"
      note: "Newer version with improved performance"
    - model_id: "NousResearch/Llama-2-7b-chat-hf"
      note: "Community fine-tune with strong instruction following"

# Cost estimation
cost_estimate:
  training_per_hour_a100: "$1.50"
  inference_per_1k_tokens: "$0.0003"
  recommended_gpu: "A100-40GB"

# Special considerations
notes:
  - "Requires HuggingFace account and acceptance of model license"
  - "Set HF_TOKEN environment variable for access"
  - "Commercial use allowed with restrictions (see license)"
