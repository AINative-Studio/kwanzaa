{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kwanzaa Adapter Training on Google Colab (Free T4 GPU)\n",
        "\n",
        "**Training Configuration:**\n",
        "- Model: meta-llama/Llama-3.2-1B-Instruct\n",
        "- Method: QLoRA (4-bit quantization)\n",
        "- Training samples: 107\n",
        "- Eval samples: 27\n",
        "- Estimated time: 15-20 minutes on free T4\n",
        "\n",
        "**Setup:**\n",
        "1. Runtime > Change runtime type > T4 GPU\n",
        "2. Upload `kwanzaa_train.jsonl` and `kwanzaa_eval.jsonl` to `/content/`\n",
        "3. Run all cells in order\n",
        "4. Download adapter from `/content/outputs/`"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q -U transformers datasets peft bitsandbytes trl accelerate huggingface_hub"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Hugging Face\n",
        "# Accept Llama 3.2 license at: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_data = load_jsonl('/content/kwanzaa_train.jsonl')\n",
        "eval_data = load_jsonl('/content/kwanzaa_eval.jsonl')\n",
        "\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "eval_dataset = Dataset.from_list(eval_data)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Eval samples: {len(eval_dataset)}\")\n",
        "print(f\"\\nSample format:\")\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure model and tokenizer\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "OUTPUT_DIR = \"/content/outputs/kwanzaa-adapter-v1\"\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(f\"\\nTrainable parameters: {model.print_trainable_parameters()}\")"
      ],
      "metadata": {
        "id": "config_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare chat template\n",
        "def format_chat_template(example):\n",
        "    messages = example[\"messages\"]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    return {\"text\": text}\n",
        "\n",
        "train_dataset = train_dataset.map(format_chat_template)\n",
        "eval_dataset = eval_dataset.map(format_chat_template)\n",
        "\n",
        "print(\"Sample formatted text:\")\n",
        "print(train_dataset[0][\"text\"][:500])"
      ],
      "metadata": {
        "id": "format_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=10,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    max_grad_norm=0.3,\n",
        "    group_by_length=True,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Data collator for completion-only training\n",
        "response_template = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "collator = DataCollatorForCompletionOnlyLM(\n",
        "    response_template=response_template,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Training configuration ready!\")"
      ],
      "metadata": {
        "id": "training_args"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize trainer and train\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training complete!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save adapter\n",
        "print(\"\\nSaving adapter...\")\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\nAdapter saved to: {OUTPUT_DIR}\")\n",
        "print(\"\\nFiles created:\")\n",
        "!ls -lh {OUTPUT_DIR}\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nDownload the adapter files from: {OUTPUT_DIR}\")\n",
        "print(\"\\nAdapter files to download:\")\n",
        "print(\"  - adapter_config.json\")\n",
        "print(\"  - adapter_model.safetensors\")\n",
        "print(\"  - tokenizer files\")"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the adapter\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "test_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a researcher assistant specializing in Kwanzaa and African American culture.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": '''Retrieved Documents:\n",
        "\n",
        "[1] Title: \"Kwanzaa Principles\"\n",
        "Content: \"The seven principles of Kwanzaa are called Nguzo Saba...\"\n",
        "\n",
        "Query: What are the seven principles of Kwanzaa?'''\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nTesting adapter:\")\n",
        "print(\"=\"*50)\n",
        "result = pipe(test_messages)\n",
        "print(result[0]['generated_text'][-1]['content'])"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Push to Hugging Face Hub\n",
        "# Uncomment to push to your HF account\n",
        "# HF_REPO_NAME = \"your-username/kwanzaa-adapter-v1\"\n",
        "#\n",
        "# trainer.model.push_to_hub(HF_REPO_NAME)\n",
        "# tokenizer.push_to_hub(HF_REPO_NAME)\n",
        "#\n",
        "# print(f\"Adapter pushed to: https://huggingface.co/{HF_REPO_NAME}\")"
      ],
      "metadata": {
        "id": "push_to_hub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
