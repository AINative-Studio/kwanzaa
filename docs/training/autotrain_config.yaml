# AutoTrain Configuration for Kwanzaa Adapter Training
# Model: Llama-3.2-1B-Instruct with QLoRA
# Training: 107 samples, Eval: 27 samples

task: llm-sft
base_model: meta-llama/Llama-3.2-1B-Instruct
project_name: kwanzaa-adapter-v1
log: tensorboard
backend: local

data:
  path: data/training
  train_split: train
  valid_split: eval
  chat_template: tokenizer  # Use model's built-in chat template

params:
  # Sequence parameters
  block_size: 2048
  model_max_length: 2048

  # Training parameters
  epochs: 3
  batch_size: 1
  gradient_accumulation: 8
  lr: 2e-4
  scheduler: cosine
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  seed: 42

  # QLoRA configuration
  peft: true
  quantization: int4
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: all-linear

  # Optimizer
  optimizer: paged_adamw_8bit

  # Precision
  fp16: false
  bf16: true

  # Logging and checkpointing
  logging_steps: 10
  eval_strategy: steps
  eval_steps: 50
  save_total_limit: 2
  save_strategy: epoch

  # Output
  push_to_hub: true
  hub_strategy: every_save
  hub_model_id: ${HF_USERNAME}/kwanzaa-adapter-v1
