# Training In Progress - Direct HF API

## Status: RUNNING

Training started at: 2026-01-20 07:31 UTC

## Method: Direct HuggingFace API (CLI)

Using transformers + TRL + PEFT directly - NO AutoTrain, NO Web UI

## Configuration

```python
Model: meta-llama/Llama-3.2-1B-Instruct
Method: QLoRA (4-bit quantization)
Training samples: 107
Eval samples: 27

LoRA Config:
- r: 16
- alpha: 32
- dropout: 0.05
- target_modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

Training:
- Epochs: 3
- Batch size: 1
- Gradient accumulation: 8
- Learning rate: 2e-4
- Scheduler: cosine
- Optimizer: paged_adamw_8bit
```

## Training Steps

1. âœ… Load training data (107 samples)
2. âœ… Load eval data (27 samples)
3. ğŸ”„ Load model with 4-bit quantization
4. â³ Apply LoRA adapters
5. â³ Format data with chat template
6. â³ Train for 3 epochs (~15-20 min)
7. â³ Save adapter

## Output Location

```
./outputs/kwanzaa-adapter-v1/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ adapter_model.safetensors  (~6MB)
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ tokenizer.json
â””â”€â”€ special_tokens_map.json
```

## Monitoring

Check training progress:
```bash
# Monitor the training script
tail -f <training_logs>
```

Training is running in background process ID: a62d53

## Estimated Time

- Model loading: 2-3 minutes
- Training: 15-20 minutes
- **Total: ~20 minutes**

## What's Next

After training completes:
1. Verify adapter files in `./outputs/kwanzaa-adapter-v1/`
2. Move to `backend/models/adapters/kwanzaa-adapter-v1/`
3. Close Issue #48 (Execute Full Training Run)
4. Complete Issue #52 (Save & Version Adapter Artifact)
5. Run evaluations (Issues #54, #56, #58, #60)

## Cost

**$0.00** - Running locally on your machine!

No cloud costs, no GPU rental fees. ğŸ‰

---

**Training command:**
```bash
source venv/bin/activate
python scripts/train_hf_direct.py
```
